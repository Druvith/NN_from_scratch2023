{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781aff36-0326-4b50-ae1e-d2e0f36057d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac6af08-8596-41f9-832a-c25dea9da385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.set_page_config(page_title=\"LIFE\", layout = \"wide\")\n",
    "st.markdown(f\"\"\"\n",
    "            <style>\n",
    "            .stApp {{background-image: url(\"https://images.unsplash.com/photo-1509537257950-20f875b03669?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1469&q=80\"); \n",
    "                     background-attachment: fixed;\n",
    "                     background-size: cover}}\n",
    "         </style>\n",
    "         \"\"\", unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0653c53a-0138-46cd-9a73-ebc76ce86faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357012c0-69d6-413b-b4a1-a6e2bda7c206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./model/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama.cpp: loading model from ./model/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 5407.71 MB (+ 2052.00 MB per state)\n",
      "...................................................AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "................................................\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(model_path = \"./model/ggml-model-q4_0.bin\")\n",
    "embeddings = LlamaCppEmbeddings(model_path = \"./model/ggml-model-q4_0.bin\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35763170-660d-473c-9859-8bb31670df0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 443, which is longer than the specified 100\n",
      "Created a chunk of size 120, which is longer than the specified 100\n",
      "Created a chunk of size 556, which is longer than the specified 100\n",
      "Created a chunk of size 364, which is longer than the specified 100\n",
      "Created a chunk of size 506, which is longer than the specified 100\n",
      "Created a chunk of size 352, which is longer than the specified 100\n",
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  2127.33 ms /     8 tokens (  265.92 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  2131.05 ms\n",
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 23448.17 ms /   107 tokens (  219.14 ms per token,     4.56 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 23486.63 ms\n",
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  7694.79 ms /    29 tokens (  265.34 ms per token,     3.77 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  7704.17 ms\n",
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 39365.42 ms /   147 tokens (  267.79 ms per token,     3.73 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 39422.41 ms\n",
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 23273.16 ms /    88 tokens (  264.47 ms per token,     3.78 tokens per second)\n",
      "llama_print_timings:        eval time =   358.50 ms /     1 runs   (  358.50 ms per token,     2.79 tokens per second)\n",
      "llama_print_timings:       total time = 23662.65 ms\n",
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 35107.67 ms /   122 tokens (  287.77 ms per token,     3.48 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 35158.44 ms\n",
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 24351.36 ms /    82 tokens (  296.97 ms per token,     3.37 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 24380.32 ms\n",
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 30992.58 ms /   115 tokens (  269.50 ms per token,     3.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 31031.12 ms\n"
     ]
    }
   ],
   "source": [
    "st.title(\"QnA\")\n",
    "\n",
    "loader = TextLoader(\"./Csscheat.txt\")\n",
    "docs = loader.load()    \n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "db = Chroma.from_documents(texts, embeddings)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663e504a-93d9-4f92-944e-9f49836ba71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2127.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     4.13 ms\n",
      "\n",
      "llama_print_timings:        load time =  1992.67 ms\n",
      "llama_print_timings:      sample time =     9.38 ms /    14 runs   (    0.67 ms per token,  1491.90 tokens per second)\n",
      "llama_print_timings: prompt eval time = 46224.17 ms /   166 tokens (  278.46 ms per token,     3.59 tokens per second)\n",
      "llama_print_timings:        eval time =  4736.19 ms /    13 runs   (  364.32 ms per token,     2.74 tokens per second)\n",
      "llama_print_timings:       total time = 51089.11 ms\n"
     ]
    }
   ],
   "source": [
    "question = st.text_input(\"Ask something related\", placeholder=\"Find something similar to: ....this.... in the text?\")    \n",
    "\n",
    "similar_doc = db.similarity_search(question, k=1)\n",
    "context = similar_doc[0].page_content\n",
    "query_llm = LLMChain(llm=llm, prompt=prompt)\n",
    "response = query_llm.run({\"context\": context, \"question\": question})        \n",
    "st.write(response)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508f962-a7dc-4dcd-8325-2bfc39d0a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2024db69-e308-4f53-96b9-48fa57422aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763acbad-10a2-4df0-b907-097dc25afceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a nice AI bot that helps a user figure out what to eat in one short sentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI like tomatoes, what should I eat?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/base.py:332\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([prompt], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    341\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "llm(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4569761-a876-4c71-8e09-21483161c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2962aa-9b7e-4998-8313-349ac7e17091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cefc6071-0f7f-4c1c-875e-4b9de8136e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1610.53 ms\n",
      "llama_print_timings:      sample time =    24.87 ms /    46 runs   (    0.54 ms per token,  1849.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1610.43 ms /     7 tokens (  230.06 ms per token,     4.35 tokens per second)\n",
      "llama_print_timings:        eval time = 12588.20 ms /    45 runs   (  279.74 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:       total time = 14377.18 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\nI am a 32 year old woman and I was told i would be getting my period on or around this date, however it is not here yet. What do you think could be the cause of this?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"WHat comes after friday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb90213-4438-4883-b4ef-98987c1d5432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
