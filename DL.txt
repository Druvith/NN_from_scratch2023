Derivative vs Gradient:

A derivative is a single number that represents the rate of change of a function at a particular point. For example, the derivative of the function f(x) = x^2 at the point x = 2 is 4. This means that the function is increasing at a rate of 4 at the point x = 2.

A gradient is a vector that represents the rate of change of a function in all directions. The gradient of the function f(x, y) = x^2 + y^2 at the point (2, 2) is the vector <4, 4>. This means that the function is increasing at a rate of 4 in the x-direction and at a rate of 4 in the y-direction at the point (2, 2).


Workflow:
1.The data that we can learn from.

2.A model of how to transform the data.

3.An objective function that quantifies how well (or badly) the model is doing.

4.An algorithm to adjust the model’s parameters to optimize the objective function	

--garbage in, garbage out

--A good rule of thumb is that any how much? or how many? problem should suggest regression


RL: 
  	Policy: a policy is just a function that maps from observations of the environment to actions. The goal of reinforcement learning is to produce good policies.
  	
Turing test:  Turing posed the question “can machines think?” in his famous paper Computing Machinery and Intelligence (Turing, 1950).

a machine can be considered intelligent if it is difficult for a human evaluator to distinguish between the replies from a machine and a human based on textual interactions.

Father of nueral nets: Donald Hebb (work - Organisation of behaviour) 

Key Points of Broadcasting:

Dimension Expansion: If two tensors have a different number of dimensions, the shape of the one with fewer dimensions is padded with ones on its left side.

Example: If you have a tensor 
�
A of shape (5, 4) and another tensor 
�
B of shape (4,), 
�
B is considered of shape (1, 4) for the operation.

Size Matching: If the size of one of the dimensions does not match, the tensor with size 1 in that dimension is stretched to match the other shape.

Example: For 
�
A of shape (5, 4) and 
�
B of shape (1, 4), 
�
B will be broadcasted to shape (5, 4).

Incompatibility: If in any dimension the sizes are different and neither is 1, an error is raised because they are incompatible for broadcasting.

Example: Tensors of shape (3, 4) and (5, 4) are incompatible.

Comma seperated values (CSV) 		

Calculus is how to increase or decrease a function value by manipulating its arguments


these basics: (i) attach gradients to those variables with respect to which we desire derivatives; (ii) record the computation of the target value; (iii) execute the backpropagation function; and (iv) access the resulting gradient.

 Bayesian probability is characterized by two unique features: (i) assigning degrees of belief to non-repeatable events, e.g., what is the probability that the moon is made out of cheese?; and (ii) subjectivity—while Bayesian probability provides unambiguous rules for how one should update their beliefs in light of new evidence, it allows for different individuals to start off with different prior beliefs. 
 
 1.Law of large numbers: Here's the pertinent information:

Weak Law of Large Numbers (WLLN): The sample average converges in probability to the expected value as the number of observations grows to infinity.

Strong Law of Large Numbers (SLLN): The sample average almost surely converges to the expected value as the number of observations grows to infinity.

 2. Central limit theorem	
 
Bayes theorem: P(H|E) = (P(E|H)P(H)/P(E)) posterior equals prior time likelihood, divided by the evidence.
 

Aleatoric uncertainity
epistemic uncertainity

wikipedia: Uncertainity quantification (https://en.wikipedia.org/wiki/Uncertainty_quantification)
 	   Chebyshev's inequality: within 2 two std from the mean we contain 75% of the value, within 3 std's we contain 88.9% of the values.
 	   
LINEAR REGRESSION:

#linearity : expected value of target can be expressed as weighted sum of features 

^ : denotes estimate value

mini batch SGD: In summary, minibatch SGD proceeds as follows: (i) initialize the values of the model parameters, typically at random; (ii) iteratively sample random minibatches from the data, updating the parameters in the direction of the negative gradient. For quadratic losses and affine transformations, this has a closed-form expansion:

common loss function for regression is squared error
generalization error is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.

The L1 norm that is calculated as the sum of the absolute values of the vector.
The L2 norm that is calculated as the square root of the sum of the squared vector values.
The max norm that is calculated as the maximum vector values

types of distribuition shift:
1.label shift
2. covariance shift
3. concept shift

MULTI LAYER PERCEPTRON (MLP)

Initialisation:
Xavier initialisation: limit : only for sigmoid or tanh activations 
	- used to diminish the vanishing and exploding gradients problem
	
	
Dynamic networks: Used when you've inputs of varying dimensionality.

during lazy initialization, unspecified dimension layers will be lazily initialised during forward pass.

INTIALISATION, NORMALISATION AND REGULARISATION ARE DIFFERENT THINGS!!!!!

CNN!!!!!!

intial layers: assigning scores to each path, regardless of the location. Translational invariance..

intial layers should focus more on local regions in the image. Locality principle.

deeper layers should be able to capture longer-range features of the image.

in cnns no. of paramters is of the order 4 X 10^6.

V is filter, convolutional kernel, weights.


Deep CNNSS:

AlexNet (krizhevsky et al)

1.eight layers
2. 5 conv layers, 2 hidden layers, 1 output layer
3. uses ReLU instead of sigmoid.
4. 1.2 m images, 60m parameters, 650,000 neurons.
5. Imagenet : 15m images, 22,000 categories.
6. patience criterian was used, when val error stopped improving. (they divided it by 10)

-Deep net with small filters outperforms shallow net with large filters. (Scale Jittering)

Batch Normalisation:
1. Preprocessing
2. Numerical stabilty
3. regularisation

various noises in optimisation often lead to faster training and less over-fitting


Dropout:
1. Preprocessing
2. Numerical stability
3. regularisation


RESNET: The idea of the resnet is that the every additional layer should more easily contain the indenity function as one of its elements.

DenseNet: Concatenating the blocks instead of adding

main components of dense net :
1. Dense Blocks: defines how inputs and outputs are concat'ed
2. Transition layers: conttols the number of channels, so that it's not too large.

Neural architecture search (NAS)
network design principles Radosavovic et al (2020)


Anynet:
1. Stem
2. Body
3. Head

RNN (Recurrent neural networks)

Sequential models: Predicting sequentially structured outputs based on sequentially structured inputs.

1. Unaligned 
2. Aligned

Unsupervised density modelling (sequence modelling)

probablity mass function: how likely we're going to see the sequence. P(x1, x2, x3, x4, .... xN)

Autoregressive models:  Such models, the regress the values of the signals on previous values of the same signals.

Latent auto-regressive models: mantinining a summary h of the previous observations, which is updated after each prediction of xt. 

Sequence modelling for language data is called language model

the model is said to satisfy markov's condition if we throw away the histoy beoynd t time steps without any loss in the predictive power.

"the future is conditionally independent of the past given it's recent history."


"make stuff which is difficult to replicate"

Freq of words tend to follow a power law distribution as we go down the ranks. (Zipfian)

Zip's law states that the frequency n. of the ith more frequent word is:

log(n) = -alog(i) + c

