{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXsY4EL0wZX9DAoB+Vyfpb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Druvith/NN_from_scratch2023/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of a three-layer CNN from scratch\n",
        "\n"
      ],
      "metadata": {
        "id": "28D9jzMCEQVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "hnjDK8LXcAfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNIST(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "      self.images = images\n",
        "      self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return self.images[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "eMh48N_PcNLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_mnist(url, path):\n",
        "  response = requests.get(url)\n",
        "  with open(path, 'wb') as f:\n",
        "    for chunk in response.iter_content(chunk_size=1024):\n",
        "      if chunk:\n",
        "        f.write(chunk)"
      ],
      "metadata": {
        "id": "Ic9PUIPKcvuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Purpose: The goal is to download, extract and convert it to a tensor for training a CNN\n",
        "\n",
        "def load_fashion_mnist():\n",
        "  base_url = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n",
        "  files = {\n",
        "        'train_images': 'train-images-idx3-ubyte.gz',\n",
        "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
        "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
        "        'test_labels': 't10k-labels-idx1-ubyte.gz'\n",
        "    }\n",
        "\n",
        "  data_dir = 'fashion_mnist_data'\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  for key, file_name in files.items():\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "      download_mnist(base_url + file_name, file_path)\n",
        "\n",
        "  train_images = extract_images(os.path.join(data_dir, files['train_images']))\n",
        "  train_labels = extract_labels(os.path.join(data_dir, files['train_labels']))\n",
        "  test_images = extract_images(os.path.join(data_dir, files['test_images']))\n",
        "  test_labels = extract_labels(os.path.join(data_dir, files['test_labels']))\n",
        "\n",
        "  train_data = FashionMNIST(train_images, train_labels)\n",
        "  test_data = FashionMNIST(test_images, test_labels)\n",
        "\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "QqgKPhI6dCE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_images(file_path):\n",
        "  with gzip.open(file_path, 'rb') as f:\n",
        "    data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "  data = data.reshape(-1, 1, 28, 28).astype(np.float32) / 255.0\n",
        "  return torch.tensor(data)"
      ],
      "metadata": {
        "id": "DuHYNoW_fQdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels(file_path):\n",
        "  with gzip.open(file_path, 'rb') as f:\n",
        "    data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "  return torch.tensor(data, dtype=torch.long)"
      ],
      "metadata": {
        "id": "cL-m7cv6gAzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = load_fashion_mnist()"
      ],
      "metadata": {
        "id": "_sOIk0EIgRjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[10]\n",
        "image.shape, label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8K-O14QgUme",
        "outputId": "a2599704-8d5c-4a14-9545-ceed370ead3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), torch.Size([]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, dataset, batch_size=1, shuffle=False):\n",
        "    self.dataset = dataset\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = shuffle\n",
        "    self.indices = np.arange(len(dataset))\n",
        "\n",
        "  def __iter__(self):\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(self.indices)\n",
        "    self.current_idx = 0\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "    if self.current_idx >= len(self.dataset):\n",
        "      raise StopIteration\n",
        "    batch_indices = self.indices[self.current_idx:self.current_idx + self.batch_size]\n",
        "    batch = [self.dataset[i] for i in batch_indices]\n",
        "    batch_images = torch.stack([item[0] for item in batch])\n",
        "    batch_labels = torch.stack([item[1] for item in batch])\n",
        "    self.current_idx += self.batch_size\n",
        "    return batch_images, batch_labels\n"
      ],
      "metadata": {
        "id": "FL2i3RR0gf3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ruoeW5jLA8",
        "outputId": "f74aadbe-b691-4de0-cda1-9ac254db2fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        init.kaiming_normal_(self.filter, mode='fan_out', nonlinearity='relu')\n",
        "    def forward(self, x):\n",
        "        # Use PyTorch's built-in function for forward pass convolution\n",
        "        out = F.conv2d(x, self.filter, stride=1, padding=0)\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_out, x, learning_rate):\n",
        "        batch_size, _, h, w = x.shape\n",
        "        grad_filter = torch.zeros_like(self.filter)\n",
        "        grad_x = torch.zeros_like(x)\n",
        "\n",
        "        out_h = h - self.kernel_size + 1\n",
        "        out_w = w - self.kernel_size + 1\n",
        "\n",
        "        for i in range(out_h):\n",
        "            for j in range(out_w):\n",
        "                region = x[:, :, i:i+self.kernel_size, j:j+self.kernel_size]\n",
        "                for k in range(self.out_channels):\n",
        "                    # Accumulate gradient for the filter\n",
        "                    grad_filter[k] += torch.sum(grad_out[:, k, i, j].view(-1, 1, 1, 1) * region, dim=0)\n",
        "\n",
        "                    # Accumulate gradient for the input\n",
        "                    grad_x[:, :, i:i+self.kernel_size, j:j+self.kernel_size] += grad_out[:, k, i, j].view(-1, 1, 1, 1) * self.filter[k]\n",
        "\n",
        "        # Update the filters using the computed gradients\n",
        "        self.filter.data -= learning_rate * grad_filter\n",
        "        return grad_x\n"
      ],
      "metadata": {
        "id": "dMPSQRrKkCwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> To create max pool operation which inherits nn.Module class (ofc) we need to 2 op vectors,  \n",
        "1. out vector - max values\n",
        "2. Indices - indices with max values. (for backward pass)\n",
        "\n",
        "> For backward pass.   \n",
        "1. Set the grad vector to be the same shape as the input.\n",
        "2. The grad ops for MaxPool is 1 for the indices which gives us the max values else 0. (since d(x)/dx = 1)\n",
        "3. Then multiply with output of the next layer, beacuse of chain rule."
      ],
      "metadata": {
        "id": "BBj0AJ91sEwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPoolLayer(nn.Module):\n",
        "  def __init__(self, kernel_size, stride=None, padding=0):\n",
        "    super(MaxPoolLayer, self).__init__()\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride if stride is not None else kernel_size\n",
        "    self.max_indices = None\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, h, w = x.shape\n",
        "    out_h = (h - self.kernel_size) // self.stride + 1\n",
        "    out_w = (w - self.kernel_size) // self.stride + 1\n",
        "\n",
        "    # Initialise out and index tensors\n",
        "    out = torch.zeros((batch_size, channels, out_h, out_w), device=x.device)\n",
        "    self.max_indices = torch.zeros_like(out, dtype=torch.long, device=x.device)\n",
        "\n",
        "    for i in range(out_h):\n",
        "      for j in range(out_w):\n",
        "        reigon = x[:, :, i*self.stride: i*self.stride +self.kernel_size, j*self.stride: j*self.stride + self.kernel_size]\n",
        "        max_values, indices = torch.max(reigon.reshape(batch_size, channels, -1), dim=2)\n",
        "        out[:, :, i, j] = max_values\n",
        "        self.max_indices[:, :, i, j] = indices\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, grad_out):\n",
        "    batch_size, channels, out_h, out_w = grad_out.shape\n",
        "    # Initialize the gradient for the input\n",
        "    grad_x = torch.zeros((batch_size, channels,\n",
        "                              out_h * self.stride,\n",
        "                              out_w * self.stride),\n",
        "                              device=grad_out.device)\n",
        "\n",
        "    for i in range(out_h):\n",
        "      for j in range(out_w):\n",
        "        region_shape = (batch_size, channels, self.kernel_size, self.kernel_size)\n",
        "        region_grad = grad_x[:, :, i*self.stride: i*self.stride + self.kernel_size, j*self.stride: j*self.stride + self.kernel_size]\n",
        "        # flatten the region to map the max indices\n",
        "        flat_region_grad = region_grad.reshape(batch_size, channels, -1)\n",
        "\n",
        "        # Use advanced indexing to map gradients back to the correct indices\n",
        "        flat_region_grad.scatter_(2, self.max_indices[:, :, i, j].unsqueeze(-1), grad_out[:, :, i, j].unsqueeze(-1))\n",
        "\n",
        "    return grad_x\n",
        "\n",
        "    ### This operation effectively backpropagates the gradient only to the positions that were the maximum in each pooling region."
      ],
      "metadata": {
        "id": "vCoVj26qpue_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Linear(nn.Module):\n",
        "  def __init__(self, in_features, out_features, device = device):\n",
        "    super(Linear, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.std = math.sqrt(2 / in_features)\n",
        "    self.weight = nn.Parameter(torch.randn(out_features, in_features) * self.std).to(device)\n",
        "    self.bias = nn.Parameter(torch.zeros(out_features)).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x @ self.weight.T + self.bias\n",
        "\n",
        "  def backward(self, grad_out, x, learning_rate):\n",
        "    grad_x = grad_out @ self.weight\n",
        "    grad_weight = grad_out.T @ x\n",
        "    grad_bias = torch.sum(grad_out, dim=0)\n",
        "\n",
        "    self.weight.data -= learning_rate * grad_weight\n",
        "    self.bias.data -= learning_rate * grad_bias\n",
        "\n",
        "    return grad_x\n"
      ],
      "metadata": {
        "id": "rhb6aPsM2bQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):     # backward pass is just resizing to the input shape\n",
        "  def forward(self, x):\n",
        "    self.input_shape = x.size()  # retain the original size\n",
        "    return x.view(x.size(0), -1)  # (B, C * H * W)\n",
        "\n",
        "  def backward(self, grad_out, x):\n",
        "    grad_x = grad_out.view(self.input_shape)\n",
        "    return grad_x\n"
      ],
      "metadata": {
        "id": "Y5j2Vzpp51zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(nn.Module):\n",
        "  def forward(self, x):\n",
        "    return torch.clamp(x, min=0)\n",
        "\n",
        "  def backward(self, grad_out, x):\n",
        "    grad_x = grad_out.clone()\n",
        "    grad_x[x <= 0] = 0\n",
        "    return grad_x"
      ],
      "metadata": {
        "id": "-HzNl8cR69-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy(nn.Module):\n",
        "  def forward(self, y_pred, y_true):\n",
        "    y_true_onehot = torch.nn.functional.one_hot(y_true, num_classes=10).float()  # Assuming 10 classes\n",
        "    probs = torch.log_softmax(y_pred, dim=1)\n",
        "    return -torch.mean(torch.sum(probs * y_true_onehot, dim=1)), probs\n",
        "\n",
        "  def backward(self, probs, y_true):\n",
        "    n = probs.shape[0]\n",
        "    grad_out = probs.clone()            #.clone() is used so not transform the original probs\n",
        "    grad_out[range(n), y_true] -= 1       # subtracts -1 from the index of current label\n",
        "    grad_out /= n\n",
        "    return grad_out"
      ],
      "metadata": {
        "id": "LvSe4yLaAlD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.conv1 = ConvLayer(1, 32, 3)\n",
        "        self.conv2 = ConvLayer(32, 64, 3)\n",
        "        self.relu = ReLU()\n",
        "        self.maxpool = MaxPoolLayer(2, stride=2)\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = Linear(64 * 12 * 12, 128)\n",
        "        self.fc2 = Linear(128, 10)\n",
        "        self.loss_fn = CrossEntropy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x1 = self.conv1(x)\n",
        "        self.x2 = self.relu(self.x1)\n",
        "        self.x3 = self.conv2(self.x2)\n",
        "        self.x4 = self.relu(self.x3)\n",
        "        self.x5 = self.maxpool(self.x4)\n",
        "        self.x6 = self.flatten(self.x5)\n",
        "        self.x7 = self.fc1(self.x6)\n",
        "        self.x8 = self.relu(self.x7)\n",
        "        self.logits = self.fc2(self.x8)\n",
        "        return self.logits\n",
        "\n",
        "    def backward(self, logits, labels,x, learning_rate):\n",
        "        loss, probs = self.loss_fn(logits, labels)\n",
        "        grad_out = self.loss_fn.backward(probs, labels)\n",
        "\n",
        "        grad_out = self.fc2.backward(grad_out, self.x8, learning_rate)\n",
        "        grad_out = self.relu.backward(grad_out, self.x7)\n",
        "        grad_out = self.fc1.backward(grad_out, self.x6, learning_rate)\n",
        "        grad_out = self.flatten.backward(grad_out, self.x5)\n",
        "        grad_out = self.maxpool.backward(grad_out)\n",
        "        grad_out = self.relu.backward(grad_out, self.x3)\n",
        "        grad_out = self.conv2.backward(grad_out, self.x2, learning_rate)\n",
        "        grad_out = self.relu.backward(grad_out, self.x1)\n",
        "        grad_out = self.conv1.backward((grad_out, x), learning_rate)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "Y2g0BBGH77u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "id": "1VztJrKOFOoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxK2vUF4FR7Q",
        "outputId": "1239828a-e3df-4768-e13c-e45bdffea657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_data_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "VohLKawtGNOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "lr = 0.001\n",
        "criterion = CrossEntropy()\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for X, y in train_data_loader:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    logits = model.forward(X)\n",
        "    loss, _ = model.loss_fn(logits, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOadeSzCFUsI",
        "outputId": "6d0030dd-1230-48da-b249-e4c83bdbf6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.4837\n",
            "Epoch 2/10, Loss: 0.7472\n",
            "Epoch 3/10, Loss: 0.7720\n",
            "Epoch 4/10, Loss: 0.3256\n",
            "Epoch 5/10, Loss: 0.7576\n",
            "Epoch 6/10, Loss: 0.7845\n",
            "Epoch 7/10, Loss: 0.4341\n",
            "Epoch 8/10, Loss: 0.2821\n",
            "Epoch 9/10, Loss: 0.4203\n",
            "Epoch 10/10, Loss: 0.2991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for X, y in test_data_loader:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    logits = model.forward(X)\n",
        "    loss, _ = model.loss_fn(logits, y)\n",
        "    print(f\"Test Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp3etQFvbwTS",
        "outputId": "7890a743-aa6c-4f37-e06a-0eb61fd26106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5973\n",
            "Test Loss: 0.5286\n",
            "Test Loss: 0.4212\n",
            "Test Loss: 0.2138\n",
            "Test Loss: 0.5401\n",
            "Test Loss: 0.2563\n",
            "Test Loss: 0.3152\n",
            "Test Loss: 0.5231\n",
            "Test Loss: 0.2496\n",
            "Test Loss: 0.3885\n",
            "Test Loss: 0.4578\n",
            "Test Loss: 0.3588\n",
            "Test Loss: 0.2452\n",
            "Test Loss: 0.1879\n",
            "Test Loss: 0.7333\n",
            "Test Loss: 0.3136\n",
            "Test Loss: 0.3138\n",
            "Test Loss: 0.6727\n",
            "Test Loss: 0.4683\n",
            "Test Loss: 0.6434\n",
            "Test Loss: 0.6153\n",
            "Test Loss: 0.5464\n",
            "Test Loss: 0.5384\n",
            "Test Loss: 0.5364\n",
            "Test Loss: 0.4255\n",
            "Test Loss: 0.4873\n",
            "Test Loss: 0.2990\n",
            "Test Loss: 0.3574\n",
            "Test Loss: 0.7512\n",
            "Test Loss: 0.4500\n",
            "Test Loss: 1.0246\n",
            "Test Loss: 0.5639\n",
            "Test Loss: 0.4548\n",
            "Test Loss: 0.3760\n",
            "Test Loss: 0.6025\n",
            "Test Loss: 0.3542\n",
            "Test Loss: 0.5193\n",
            "Test Loss: 0.5195\n",
            "Test Loss: 0.6590\n",
            "Test Loss: 0.4839\n",
            "Test Loss: 0.3324\n",
            "Test Loss: 0.5411\n",
            "Test Loss: 0.3482\n",
            "Test Loss: 0.5069\n",
            "Test Loss: 0.2894\n",
            "Test Loss: 0.6433\n",
            "Test Loss: 0.3550\n",
            "Test Loss: 0.4446\n",
            "Test Loss: 0.4157\n",
            "Test Loss: 0.3178\n",
            "Test Loss: 0.3936\n",
            "Test Loss: 0.6566\n",
            "Test Loss: 0.2970\n",
            "Test Loss: 0.4231\n",
            "Test Loss: 0.5133\n",
            "Test Loss: 0.3677\n",
            "Test Loss: 0.3005\n",
            "Test Loss: 0.5473\n",
            "Test Loss: 0.3319\n",
            "Test Loss: 0.3040\n",
            "Test Loss: 0.4217\n",
            "Test Loss: 0.8542\n",
            "Test Loss: 0.3841\n",
            "Test Loss: 0.6766\n",
            "Test Loss: 0.3471\n",
            "Test Loss: 0.4145\n",
            "Test Loss: 0.1812\n",
            "Test Loss: 0.3060\n",
            "Test Loss: 0.4080\n",
            "Test Loss: 0.3729\n",
            "Test Loss: 0.2068\n",
            "Test Loss: 0.3542\n",
            "Test Loss: 0.4737\n",
            "Test Loss: 0.3182\n",
            "Test Loss: 0.5063\n",
            "Test Loss: 0.2450\n",
            "Test Loss: 0.3504\n",
            "Test Loss: 0.7291\n",
            "Test Loss: 0.6286\n",
            "Test Loss: 0.3869\n",
            "Test Loss: 0.5409\n",
            "Test Loss: 0.6351\n",
            "Test Loss: 0.4928\n",
            "Test Loss: 0.5288\n",
            "Test Loss: 0.4614\n",
            "Test Loss: 0.5697\n",
            "Test Loss: 0.2984\n",
            "Test Loss: 0.2671\n",
            "Test Loss: 0.6054\n",
            "Test Loss: 0.5187\n",
            "Test Loss: 1.0958\n",
            "Test Loss: 0.9016\n",
            "Test Loss: 0.4751\n",
            "Test Loss: 0.5727\n",
            "Test Loss: 0.4501\n",
            "Test Loss: 0.3889\n",
            "Test Loss: 0.7288\n",
            "Test Loss: 0.4110\n",
            "Test Loss: 0.2846\n",
            "Test Loss: 0.6190\n",
            "Test Loss: 0.4065\n",
            "Test Loss: 1.0902\n",
            "Test Loss: 0.5575\n",
            "Test Loss: 0.7567\n",
            "Test Loss: 0.4128\n",
            "Test Loss: 0.2113\n",
            "Test Loss: 0.2435\n",
            "Test Loss: 0.3700\n",
            "Test Loss: 0.9382\n",
            "Test Loss: 0.6432\n",
            "Test Loss: 0.5769\n",
            "Test Loss: 0.3188\n",
            "Test Loss: 0.2434\n",
            "Test Loss: 0.3798\n",
            "Test Loss: 0.5216\n",
            "Test Loss: 0.5205\n",
            "Test Loss: 0.3608\n",
            "Test Loss: 0.3702\n",
            "Test Loss: 0.5692\n",
            "Test Loss: 0.9491\n",
            "Test Loss: 0.3823\n",
            "Test Loss: 0.5681\n",
            "Test Loss: 0.4382\n",
            "Test Loss: 0.4272\n",
            "Test Loss: 0.7006\n",
            "Test Loss: 0.5046\n",
            "Test Loss: 0.5768\n",
            "Test Loss: 0.5771\n",
            "Test Loss: 0.4208\n",
            "Test Loss: 0.7616\n",
            "Test Loss: 0.3454\n",
            "Test Loss: 0.5161\n",
            "Test Loss: 0.5072\n",
            "Test Loss: 0.2684\n",
            "Test Loss: 0.3482\n",
            "Test Loss: 0.3646\n",
            "Test Loss: 0.3500\n",
            "Test Loss: 0.6755\n",
            "Test Loss: 0.4427\n",
            "Test Loss: 0.3158\n",
            "Test Loss: 0.6900\n",
            "Test Loss: 0.2090\n",
            "Test Loss: 0.2903\n",
            "Test Loss: 0.4751\n",
            "Test Loss: 0.4268\n",
            "Test Loss: 0.8735\n",
            "Test Loss: 0.4168\n",
            "Test Loss: 0.3383\n",
            "Test Loss: 0.3729\n",
            "Test Loss: 0.2546\n",
            "Test Loss: 0.7197\n",
            "Test Loss: 0.7170\n",
            "Test Loss: 0.3687\n",
            "Test Loss: 0.6559\n",
            "Test Loss: 0.4623\n",
            "Test Loss: 0.2657\n",
            "Test Loss: 0.6073\n",
            "Test Loss: 0.7304\n",
            "Test Loss: 0.7332\n",
            "Test Loss: 0.5877\n",
            "Test Loss: 0.6032\n",
            "Test Loss: 0.5181\n",
            "Test Loss: 0.6514\n",
            "Test Loss: 0.3175\n",
            "Test Loss: 0.5905\n",
            "Test Loss: 0.3027\n",
            "Test Loss: 0.6803\n",
            "Test Loss: 0.1699\n",
            "Test Loss: 0.3452\n",
            "Test Loss: 0.3663\n",
            "Test Loss: 0.3624\n",
            "Test Loss: 0.5675\n",
            "Test Loss: 0.9397\n",
            "Test Loss: 0.4279\n",
            "Test Loss: 0.7431\n",
            "Test Loss: 0.4562\n",
            "Test Loss: 0.5698\n",
            "Test Loss: 0.3986\n",
            "Test Loss: 0.4559\n",
            "Test Loss: 0.2113\n",
            "Test Loss: 0.3296\n",
            "Test Loss: 0.5351\n",
            "Test Loss: 0.4340\n",
            "Test Loss: 0.3176\n",
            "Test Loss: 0.4108\n",
            "Test Loss: 0.2847\n",
            "Test Loss: 0.5712\n",
            "Test Loss: 0.5429\n",
            "Test Loss: 0.4614\n",
            "Test Loss: 0.3902\n",
            "Test Loss: 0.5135\n",
            "Test Loss: 0.5473\n",
            "Test Loss: 0.5526\n",
            "Test Loss: 0.3938\n",
            "Test Loss: 0.3797\n",
            "Test Loss: 0.4229\n",
            "Test Loss: 0.5167\n",
            "Test Loss: 0.4200\n",
            "Test Loss: 0.4106\n",
            "Test Loss: 0.3257\n",
            "Test Loss: 0.4891\n",
            "Test Loss: 0.5684\n",
            "Test Loss: 0.5593\n",
            "Test Loss: 0.3981\n",
            "Test Loss: 0.5078\n",
            "Test Loss: 0.5525\n",
            "Test Loss: 0.5010\n",
            "Test Loss: 0.4590\n",
            "Test Loss: 0.6387\n",
            "Test Loss: 0.5613\n",
            "Test Loss: 0.3724\n",
            "Test Loss: 0.3817\n",
            "Test Loss: 0.4293\n",
            "Test Loss: 0.3240\n",
            "Test Loss: 0.7015\n",
            "Test Loss: 0.4524\n",
            "Test Loss: 0.4153\n",
            "Test Loss: 0.4799\n",
            "Test Loss: 0.5417\n",
            "Test Loss: 0.3874\n",
            "Test Loss: 0.4462\n",
            "Test Loss: 0.4883\n",
            "Test Loss: 0.5136\n",
            "Test Loss: 0.3498\n",
            "Test Loss: 0.4951\n",
            "Test Loss: 0.3782\n",
            "Test Loss: 0.3743\n",
            "Test Loss: 0.2575\n",
            "Test Loss: 0.3585\n",
            "Test Loss: 0.6296\n",
            "Test Loss: 0.2980\n",
            "Test Loss: 0.1918\n",
            "Test Loss: 0.4266\n",
            "Test Loss: 0.3324\n",
            "Test Loss: 0.2808\n",
            "Test Loss: 0.5770\n",
            "Test Loss: 0.5170\n",
            "Test Loss: 0.3605\n",
            "Test Loss: 0.4942\n",
            "Test Loss: 0.4234\n",
            "Test Loss: 0.2756\n",
            "Test Loss: 0.4570\n",
            "Test Loss: 0.3497\n",
            "Test Loss: 0.2755\n",
            "Test Loss: 0.5462\n",
            "Test Loss: 0.4620\n",
            "Test Loss: 0.4880\n",
            "Test Loss: 0.3841\n",
            "Test Loss: 0.6582\n",
            "Test Loss: 0.6235\n",
            "Test Loss: 0.4259\n",
            "Test Loss: 0.4529\n",
            "Test Loss: 0.5137\n",
            "Test Loss: 0.4096\n",
            "Test Loss: 0.6636\n",
            "Test Loss: 0.3144\n",
            "Test Loss: 0.4435\n",
            "Test Loss: 0.1930\n",
            "Test Loss: 0.3441\n",
            "Test Loss: 0.4009\n",
            "Test Loss: 0.3712\n",
            "Test Loss: 0.2586\n",
            "Test Loss: 0.3383\n",
            "Test Loss: 0.2524\n",
            "Test Loss: 0.5088\n",
            "Test Loss: 0.2525\n",
            "Test Loss: 0.4634\n",
            "Test Loss: 0.2605\n",
            "Test Loss: 0.3827\n",
            "Test Loss: 0.6710\n",
            "Test Loss: 0.5303\n",
            "Test Loss: 0.3389\n",
            "Test Loss: 0.5836\n",
            "Test Loss: 0.7614\n",
            "Test Loss: 0.7560\n",
            "Test Loss: 0.4891\n",
            "Test Loss: 0.2996\n",
            "Test Loss: 0.6102\n",
            "Test Loss: 0.5171\n",
            "Test Loss: 0.7484\n",
            "Test Loss: 0.2924\n",
            "Test Loss: 0.3322\n",
            "Test Loss: 0.2359\n",
            "Test Loss: 0.5709\n",
            "Test Loss: 0.4240\n",
            "Test Loss: 0.6935\n",
            "Test Loss: 0.7005\n",
            "Test Loss: 0.5371\n",
            "Test Loss: 0.4726\n",
            "Test Loss: 0.5302\n",
            "Test Loss: 0.4604\n",
            "Test Loss: 0.4336\n",
            "Test Loss: 0.5012\n",
            "Test Loss: 0.2423\n",
            "Test Loss: 0.1375\n",
            "Test Loss: 0.6291\n",
            "Test Loss: 0.7264\n",
            "Test Loss: 0.2295\n",
            "Test Loss: 0.5267\n",
            "Test Loss: 0.5333\n",
            "Test Loss: 0.5051\n",
            "Test Loss: 0.3996\n",
            "Test Loss: 0.5882\n",
            "Test Loss: 0.3547\n",
            "Test Loss: 0.3195\n",
            "Test Loss: 0.2754\n",
            "Test Loss: 0.4377\n",
            "Test Loss: 0.2111\n",
            "Test Loss: 0.5384\n",
            "Test Loss: 0.4187\n",
            "Test Loss: 0.5993\n",
            "Test Loss: 0.4711\n",
            "Test Loss: 0.3389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "# Iterate over the first 10 samples of the test data\n",
        "for i in range(20,30):\n",
        "    x, y = test_data[i]  # Unpack the input and label for each sample\n",
        "    x = x.to(device)\n",
        "    x = x.unsqueeze(0)  # Add a batch dimension\n",
        "    with torch.no_grad():\n",
        "        pred = model.forward(x)\n",
        "        predicted, actual = classes[pred[0].argmax()], classes[y]\n",
        "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VYDFCkkc7Cg",
        "outputId": "0affa788-1bb9-490b-fb83-617357478854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Pullover\", Actual: \"Pullover\"\n",
            "Predicted: \"Sneaker\", Actual: \"Sandal\"\n",
            "Predicted: \"Sneaker\", Actual: \"Sneaker\"\n",
            "Predicted: \"Sandal\", Actual: \"Ankle boot\"\n",
            "Predicted: \"Trouser\", Actual: \"Trouser\"\n",
            "Predicted: \"Pullover\", Actual: \"Coat\"\n",
            "Predicted: \"Shirt\", Actual: \"Shirt\"\n",
            "Predicted: \"T-shirt/top\", Actual: \"T-shirt/top\"\n",
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n",
            "Predicted: \"Dress\", Actual: \"Dress\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "EKusakLISt7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udW6ejptTjjX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}