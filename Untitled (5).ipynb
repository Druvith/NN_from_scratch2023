{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd811a55-37ec-4595-8e6e-b59f8a1910d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, fsspec, huggingface_hub\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2023.12.2 huggingface_hub-0.20.3 tqdm-4.66.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0ec674-3bee-4d04-8e52-1212f210b73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02441223313544ffb6ca0aaa49ad1a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8aa6b86a9340758aa474bd8a551327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750dbe3c106c4150a49dc68b2f5ed685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/626 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d425298178432c99422e7b7d82da44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cfdc6073b64ef18bb7a196535bf4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0f669fc01e4807bf00dbc420b4b987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45c8c701bb348b59d7e5c511f5f9724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0508b7d0b4d148d2976bc696cefd0bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ce6c780f584d82b0d4b781872ddcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d96e915ef4b4ee9921e73870a5bb266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737ce07a00b84425841a7f03f5716156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e5f3a7f36b4127ab635f1137716386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e57bf8ee2e245c49c53b6a1267902e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6aa9826c1148d4aa7e5e77482f9f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfbd82cb9d14231ae3f60645e86eb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ee7503752e44c59cd8fcb23e35b4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/mistralmed'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_id=\"Druvith/MedQA1\"\n",
    "snapshot_download(repo_id=model_id, local_dir=\"mistralmed\",\n",
    "                  local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b161be-95d8-4d23-a984-84188ae24e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 17679, done.\u001b[K\n",
      "remote: Counting objects: 100% (6154/6154), done.\u001b[K\n",
      "remote: Compressing objects: 100% (280/280), done.\u001b[K\n",
      "remote: Total 17679 (delta 5999), reused 5915 (delta 5874), pack-reused 11525\u001b[K\n",
      "Receiving objects: 100% (17679/17679), 20.65 MiB | 11.74 MiB/s, done.\n",
      "Resolving deltas: 100% (12367/12367), done.\n",
      "Updating files: 100% (467/467), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0c2557-46b7-4705-a013-efdbb2d191eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy~=1.24.4 (from -r llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting sentencepiece~=0.1.98 (from -r llama.cpp/./requirements/requirements-convert.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.35.2 (from -r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
      "  Downloading gguf-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.1.1 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.12.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
      "Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m188.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m165.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, safetensors, regex, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, tokenizers, nvidia-cusolver-cu12, transformers, torch\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.2 regex-2023.12.25 safetensors-0.4.2 sentencepiece-0.1.99 tokenizers-0.15.1 torch-2.1.2 transformers-4.37.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5cb036e-7ec6-40e6-a3bf-8bb60da7a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file mistralmed/pytorch_model-00001-of-00003.bin\n",
      "Loading model file mistralmed/pytorch_model-00001-of-00003.bin\n",
      "Loading model file mistralmed/pytorch_model-00002-of-00003.bin\n",
      "Loading model file mistralmed/pytorch_model-00003-of-00003.bin\n",
      "params = Params(n_vocab=32002, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('mistralmed'))\n",
      "Found vocab files: {'tokenizer.model': PosixPath('mistralmed/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': PosixPath('mistralmed/tokenizer.json')}\n",
      "Loading vocab file 'mistralmed/tokenizer.model', type 'spm'\n",
      "Vocab info: <SentencePieceVocab with 32000 base tokens and 2 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 32000, 'unk': 0, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32002, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32002, 4096]\n",
      "Writing mistralmed-7b-v1.5.gguf, format 7\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 32000\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "gguf: Setting chat_template to {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "/workspace/llama.cpp/convert.py:99: RuntimeWarning: invalid value encountered in divide\n",
      "  qs = (blocks / d[:, None]).round()\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32002 x   4096  | type Q8_0 | T+   7\n",
      "[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   8\n",
      "[  3/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+   8\n",
      "[  4/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+   8\n",
      "[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   8\n",
      "[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+   8\n",
      "[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+   8\n",
      "[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+   8\n",
      "[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   8\n",
      "[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   9\n",
      "[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+   9\n",
      "[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+   9\n",
      "[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   9\n",
      "[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  11\n",
      "[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  11\n",
      "[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  12\n",
      "[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  12\n",
      "[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  12\n",
      "[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  12\n",
      "[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  14\n",
      "[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  15\n",
      "[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  15\n",
      "[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  15\n",
      "[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  15\n",
      "[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  15\n",
      "[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  15\n",
      "[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  15\n",
      "[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  15\n",
      "[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  17\n",
      "[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  18\n",
      "[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  18\n",
      "[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  19\n",
      "[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  19\n",
      "[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  19\n",
      "[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  19\n",
      "[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  20\n",
      "[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  21\n",
      "[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  22\n",
      "[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  22\n",
      "[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  22\n",
      "[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  22\n",
      "[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  22\n",
      "[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  22\n",
      "[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  22\n",
      "[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  24\n",
      "[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  24\n",
      "[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  25\n",
      "[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  25\n",
      "[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  25\n",
      "[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  25\n",
      "[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  27\n",
      "[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  27\n",
      "[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  28\n",
      "[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  28\n",
      "[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  28\n",
      "[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  28\n",
      "[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  28\n",
      "[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  28\n",
      "[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  28\n",
      "[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  30\n",
      "[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  31\n",
      "[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  31\n",
      "[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  31\n",
      "[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  31\n",
      "[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  31\n",
      "[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  31\n",
      "[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  31\n",
      "[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  31\n",
      "[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  33\n",
      "[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  34\n",
      "[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  34\n",
      "[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  34\n",
      "[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  34\n",
      "[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  34\n",
      "[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type Q8_0 | T+  35\n",
      "[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type Q8_0 | T+  35\n",
      "[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  35\n",
      "[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type Q8_0 | T+  36\n",
      "[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type Q8_0 | T+  37\n",
      "[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type Q8_0 | T+  37\n",
      "[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  38\n",
      "[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  38\n",
      "[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  38\n",
      "[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  38\n",
      "[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  38\n",
      "[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  38\n",
      "[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  39\n",
      "[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  40\n",
      "[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  40\n",
      "[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  41\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  41\n",
      "[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[102/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  41\n",
      "[103/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  41\n",
      "[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  43\n",
      "[106/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  43\n",
      "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  44\n",
      "[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  44\n",
      "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  44\n",
      "[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  44\n",
      "[111/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  44\n",
      "[112/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  44\n",
      "[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  44\n",
      "[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  46\n",
      "[115/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  46\n",
      "[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  48\n",
      "[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  48\n",
      "[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  48\n",
      "[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  48\n",
      "[120/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  49\n",
      "[121/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  49\n",
      "[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  49\n",
      "[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  49\n",
      "[124/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  51\n",
      "[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  52\n",
      "[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  54\n",
      "[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  54\n",
      "[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  55\n",
      "[129/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  55\n",
      "[130/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  55\n",
      "[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  55\n",
      "[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  55\n",
      "[133/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  56\n",
      "[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  57\n",
      "[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  57\n",
      "[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  57\n",
      "[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[138/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  58\n",
      "[139/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  58\n",
      "[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  58\n",
      "[142/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  60\n",
      "[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  60\n",
      "[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  60\n",
      "[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  60\n",
      "[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  60\n",
      "[147/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[148/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  61\n",
      "[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  62\n",
      "[151/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  63\n",
      "[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  63\n",
      "[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  64\n",
      "[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  64\n",
      "[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  64\n",
      "[156/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  64\n",
      "[157/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  64\n",
      "[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  64\n",
      "[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  65\n",
      "[160/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  66\n",
      "[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  66\n",
      "[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  67\n",
      "[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  67\n",
      "[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  67\n",
      "[165/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  67\n",
      "[166/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  67\n",
      "[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  67\n",
      "[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  68\n",
      "[169/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  69\n",
      "[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  69\n",
      "[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  70\n",
      "[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  70\n",
      "[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  70\n",
      "[174/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  70\n",
      "[175/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  70\n",
      "[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  70\n",
      "[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  72\n",
      "[178/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  72\n",
      "[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  73\n",
      "[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  73\n",
      "[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  73\n",
      "[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[183/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  73\n",
      "[184/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  73\n",
      "[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  75\n",
      "[187/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  75\n",
      "[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  76\n",
      "[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  76\n",
      "[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  76\n",
      "[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  76\n",
      "[192/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  76\n",
      "[193/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  76\n",
      "[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  76\n",
      "[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  78\n",
      "[196/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  78\n",
      "[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  79\n",
      "[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  79\n",
      "[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  79\n",
      "[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  79\n",
      "[201/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  79\n",
      "[202/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  79\n",
      "[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  79\n",
      "[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  81\n",
      "[205/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  81\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  82\n",
      "[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  82\n",
      "[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  82\n",
      "[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  82\n",
      "[210/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  82\n",
      "[211/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  82\n",
      "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  82\n",
      "[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  84\n",
      "[214/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  84\n",
      "[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  85\n",
      "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  85\n",
      "[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  85\n",
      "[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  85\n",
      "[219/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  85\n",
      "[220/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  85\n",
      "[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  85\n",
      "[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  87\n",
      "[223/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  87\n",
      "[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  88\n",
      "[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  88\n",
      "[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  88\n",
      "[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  88\n",
      "[228/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  88\n",
      "[229/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  88\n",
      "[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  88\n",
      "[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  90\n",
      "[232/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  91\n",
      "[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  91\n",
      "[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  92\n",
      "[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  92\n",
      "[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  92\n",
      "[237/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  92\n",
      "[238/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  92\n",
      "[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  92\n",
      "[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  93\n",
      "[241/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  94\n",
      "[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  94\n",
      "[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  95\n",
      "[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  95\n",
      "[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  95\n",
      "[246/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  95\n",
      "[247/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  95\n",
      "[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  95\n",
      "[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+  97\n",
      "[250/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+  97\n",
      "[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+  98\n",
      "[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  98\n",
      "[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  98\n",
      "[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  98\n",
      "[255/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+  98\n",
      "[256/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+  98\n",
      "[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  98\n",
      "[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 100\n",
      "[259/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 100\n",
      "[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 101\n",
      "[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 101\n",
      "[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 101\n",
      "[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 101\n",
      "[264/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 101\n",
      "[265/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 101\n",
      "[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 101\n",
      "[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 103\n",
      "[268/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 104\n",
      "[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 104\n",
      "[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 105\n",
      "[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 105\n",
      "[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 105\n",
      "[273/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 105\n",
      "[274/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 105\n",
      "[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 105\n",
      "[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 106\n",
      "[277/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 107\n",
      "[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 107\n",
      "[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 108\n",
      "[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 108\n",
      "[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 108\n",
      "[282/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type Q8_0 | T+ 108\n",
      "[283/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type Q8_0 | T+ 108\n",
      "[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 108\n",
      "[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type Q8_0 | T+ 109\n",
      "[286/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type Q8_0 | T+ 110\n",
      "[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type Q8_0 | T+ 110\n",
      "[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 110\n",
      "[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 110\n",
      "[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 110\n",
      "[291/291] Writing tensor output.weight                          | size  32002 x   4096  | type Q8_0 | T+ 113\n",
      "Wrote mistralmed-7b-v1.5.gguf\n"
     ]
    }
   ],
   "source": [
    "!python3 llama.cpp/convert.py mistralmed \\\n",
    "  --outfile mistralmed-7b-v1.5.gguf \\\n",
    "  --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b644b-fb57-449c-9eea-cc4bd40f3a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037ca46366ee41aaac0b478f22eb5a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistralmed-7b-v1.5.gguf:   0%|          | 0.00/7.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "HUGGING_FACE_HUB_TOKEN=\"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\"\n",
    "api = HfApi(token = HUGGING_FACE_HUB_TOKEN)\n",
    "\n",
    "model_id = \"Druvith/mistralmed-7b-v1.5.gguf\"\n",
    "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"mistralmed-7b-v1.5.gguf\",\n",
    "    path_in_repo=\"mistralmed-7b-v2.0.gguf\",\n",
    "    repo_id=model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8191ad-5d06-452c-a11e-0fa7aa15e364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
